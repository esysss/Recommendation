{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4345021-0cf4-4067-808d-35f7b615368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esysss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Esysss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:31: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  return th.device('cpu')\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aabec8dc-9cc7-4f70-80ad-b56724643628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite(T_train):\n",
    "    user, item = np.nonzero(T_train)\n",
    "\n",
    "    # G = nx.Graph()\n",
    "    G = nx.DiGraph()\n",
    "    for i, j in zip(user, item):\n",
    "        G.add_edge(\"user\"+ str(i) ,\"item\"+ str(j), weight=T_train[i,j])\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c91d1941-25d3-4bd5-a744-c3990caf9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('train.npy')\n",
    "test = np.load('test.npy')\n",
    "\n",
    "T = train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7062eeac-0721-4554-ad55-2219e0227c67",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of numpy.int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m graph \u001b[38;5;241m=\u001b[39m bipartite(T)\n\u001b[1;32m----> 3\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mlen\u001b[39m(graph\u001b[38;5;241m.\u001b[39mnodes()), \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\convert.py:1347\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[1;34m(nx_graph, node_attrs, edge_attrs, edge_id_attr_name, idtype, device)\u001b[0m\n\u001b[0;32m   1345\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot all edges have attribute \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(attr))\n\u001b[1;32m-> 1347\u001b[0m         g\u001b[38;5;241m.\u001b[39medata[attr] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcopy_to(\u001b[43m_batcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, g\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\convert.py:1312\u001b[0m, in \u001b[0;36mfrom_networkx.<locals>._batcher\u001b[1;34m(lst)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcat([F\u001b[38;5;241m.\u001b[39munsqueeze(x, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lst], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:44\u001b[0m, in \u001b[0;36mtensor\u001b[1;34m(data, dtype)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mas_tensor(data, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of numpy.int32"
     ]
    }
   ],
   "source": [
    "graph = bipartite(T)\n",
    "\n",
    "g = dgl.from_networkx(graph, edge_attrs=['weight'])\n",
    "g.ndata['feat'] = torch.rand(len(graph.nodes()), 5)\n",
    "\n",
    "import pickle\n",
    "file = open('dpp.p','wb')\n",
    "pickle.dump(g.edata,file)\n",
    "file.close()\n",
    "\n",
    "\"\"\"## Graph Visualization\n",
    "\n",
    "We use the networkx library for visualizing the friendship relationships. There are hundreds of nodes (not shown), and thousands of links, so it can seem a bit messy when the graph is drawn.\n",
    "\"\"\"\n",
    "\n",
    "TEST_RATIO = 0.1  # ratio for splitting the test set\n",
    "\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * TEST_RATIO)  # number of edges in test set\n",
    "train_size = g.number_of_edges() - test_size  # number of edges in train set\n",
    "\n",
    "# get positive edges for test and train\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "# Find all negative edges\n",
    "adj = nx.adjacency_matrix(graph)\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "# split the negative edges for training and testing \n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "\n",
    "# construct positive and negative graphs for training and testing\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "# training graph\n",
    "train_g = dgl.remove_edges(g, eids[:test_size])\n",
    "train_g = dgl.add_self_loop(train_g)\n",
    "\n",
    "\"\"\"## Model definition\n",
    "\n",
    "Here we build two models from the existing implementation in the DGL library. Both models consists of 2 convolutional layers. \n",
    "\n",
    "`GCN` is a standard graph convolutional network, and serves as a baseline. `GraphSAGE` is a more advanced and powerful network that we're testing.\n",
    "\n",
    "We also define the `DotPredictor`, which helps to predict the likelihood of whether an edge exists or not, by taking the dot product of two nodes. \n",
    "\"\"\"\n",
    "\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "# ----------- create model -------------- #\n",
    "# build an ordinary GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, h_feats)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]\n",
    "\n",
    "\"\"\"## Training and testing pipeline\n",
    "\n",
    "Here, we define a pipeline for training our model, and evaluating its performance.\n",
    "\n",
    "For evaluation metric, we use AUC, which is explained in more detail in the blog post. \n",
    "\n",
    "---\n",
    "Note: In the blog post, we mentioned using ranking-based metrics such as `HitRate@K`. However, such kind of metric cannot be used in our colab implementation, since we randomly split the testing edges from the training edges. This means that some users will not have an friend in the test set, and they will never receive a hit when measuring `HitRate@K`. To solve this, we could have tried to split our dataset in a better way, if given more time.\n",
    "\"\"\"\n",
    "\n",
    "def pipeline(model_name='GCN', hidden_size=16):\n",
    "    # model_name can be GCN or SAGE\n",
    "    # hidden_size is the size of the hidden layer in the neural net\n",
    "    if model_name == 'GCN':\n",
    "        model = GCN(train_g.ndata['feat'].shape[1], hidden_size)\n",
    "    elif model_name == 'SAGE':\n",
    "        model = GraphSAGE(train_g.ndata['feat'].shape[1], hidden_size)\n",
    "        \n",
    "    pred = DotPredictor()\n",
    "\n",
    "    def compute_loss(pos_score, neg_score):  # computes the loss based on binary cross entropy\n",
    "        scores = torch.cat([pos_score, neg_score])\n",
    "        labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "    def compute_auc(pos_score, neg_score):  # computes AUC (Area-Under-Curve) score\n",
    "        scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "        labels = torch.cat(\n",
    "            [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "        return roc_auc_score(labels, scores)\n",
    "\n",
    "\n",
    "    # ----------- set up loss and optimizer -------------- #\n",
    "    # in this case, loss will in training loop\n",
    "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "\n",
    "    # ----------- training -------------------------------- #\n",
    "    all_logits = []\n",
    "    for e in range(200):\n",
    "        # forward\n",
    "        h = model(train_g, train_g.ndata['feat'])  # get node embeddings\n",
    "        pos_score = pred(train_pos_g, h)\n",
    "        neg_score = pred(train_neg_g, h)\n",
    "        loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {}'.format(e, loss))\n",
    "\n",
    "    # ----------- test and check results ---------------- #\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    with torch.no_grad():\n",
    "        pos_score = pred(test_pos_g, h)\n",
    "        neg_score = pred(test_neg_g, h)\n",
    "        print('AUC', compute_auc(pos_score, neg_score))\n",
    "    \n",
    "    return h  # return node embeddings\n",
    "\n",
    "\"\"\"### Training and testing GCN:\"\"\"\n",
    "\n",
    "h = pipeline(\"GCN\")\n",
    "\n",
    "\"\"\"### Training and testing SAGE:\"\"\"\n",
    "\n",
    "h1 = pipeline(\"SAGE\")\n",
    "\n",
    "\"\"\"# Friend recommendation\n",
    "\n",
    "Once we trained our model, we can finally suggest friends. To do this, we calculate the embedding dot products between a given user and all other users that he/she is currently NOT friends with. Then we choose 5 users with the highest dot product scores to recommend to him/her.\n",
    "\"\"\"\n",
    "\n",
    "# --------- generate recommendation for user -------- #\n",
    "num_nodes = len(graph.nodes())\n",
    "nodes = list(graph.nodes())\n",
    "def generate_rec(h, user_id=0):\n",
    "    # `h` represents the node embeddings, with shape [num_nodes, hidden_size]\n",
    "\n",
    "    # generate a graph with (num_nodes - num_friends_of_user) edges\n",
    "    # one end of the edge is user_id\n",
    "    # the other end is a user that's NOT friends with user_id\n",
    "    user_friends = set()\n",
    "    user_neg_u, user_neg_v = [], []\n",
    "    for n1, n2 in zip(u, v):   # get all friends of user_id\n",
    "        if int(n1) == user_id:\n",
    "            user_friends.add(int(n2))\n",
    "        if int(n2) == user_id:\n",
    "            user_friends.add(int(n1))\n",
    "    \n",
    "    for i in range(num_nodes):  # generate \"negative edges\" for user_id\n",
    "        if i != user_id and i not in user_friends:\n",
    "            user_neg_u.append(user_id)\n",
    "            user_neg_v.append(i)\n",
    "\n",
    "    user_g = dgl.graph((user_neg_u, user_neg_v), num_nodes=g.number_of_nodes())\n",
    "    \n",
    "    pred = DotPredictor()\n",
    "    \n",
    "    # calculate the score of each user\n",
    "    scores = [(i, score) for i, score in enumerate(pred(user_g, h).detach().numpy())]\n",
    "\n",
    "    # produce final ranked list\n",
    "    scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "    # display results\n",
    "    # print(f\"List of 5 suggested friends for user {user_id}:\")\n",
    "    # r = []\n",
    "    # for i in range(5):\n",
    "    #   # print(f'- User {scores[i][0]}, score = {scores[i][1]}')\n",
    "    #   # print(nodes[scores[i][0]])\n",
    "    #   r.append(nodes[scores[i][0]])\n",
    "    return scores\n",
    "\n",
    "\"\"\"The recommended users are shown as follows:\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "user,item = np.nonzero(T_test)\n",
    "pred = []\n",
    "real = []\n",
    "for i,j in tqdm(zip(user, item),total = len(user)):\n",
    "    us = nodes.index('user{}'.format(i))\n",
    "    item = nodes.index('item{}'.format(j))\n",
    "\n",
    "    temp_rec = np.array(generate_rec(h1, user_id=us))\n",
    "    try:\n",
    "      ad = np.where(temp_rec[:,0]==item)[0][0]\n",
    "      pred.append(temp_rec[ad,1])\n",
    "    except:\n",
    "      pred.append(0)\n",
    "    if T_test[i,j] > 2:\n",
    "        real.append(1)\n",
    "    else:\n",
    "        real.append(0)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred = np.array(pred)\n",
    "real = np.array(real)\n",
    "p = pred.copy()\n",
    "\n",
    "threshold = 1\n",
    "p[pred <= threshold] = 1\n",
    "p[pred > threshold] = 0\n",
    "\n",
    "print(accuracy_score(real,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01381d59-aee9-4cd7-b182-46ccee1a6e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
